#pragma once
/**
 * Blockwise reduce
 * it only reduces to CEIL_DIV(len,2*B_SIZE)
 */

/**
 * MAP
 */
template<typename FUNC,typename M, typename N>
__device__ void mapFuncKernel(FUNC f, const M* input, N* output, size_t len){
	int pos = B_SIZE * blockIdx.x + threadIdx.x;
	if(pos < len)
		output[pos] = f(input[pos]);
}

template<typename FUNC,typename M, typename N>
void mapFunc(FUNC f, const M* input, N* output, size_t len){
	f<<<CEIL_DIV(len,B_SIZE),B_SIZE>>>(input,output,len);
}

/**
 * REDUCE
 */
template<typename FUNC, typename N>
__device__ void reduceFuncKernel(FUNC f, const N* input, N* output, N neutralValue, size_t len){
	__shared__ double partialSum[2*B_SIZE];
	unsigned int t = threadIdx.x;
	unsigned int start = 2*blockIdx.x*blockDim.x;

	//load Segments into shared memory
	start+t<len?
		partialSum[t]=input[start+t]:
		partialSum[t]=neutralValue;
	start+blockDim.x+t<len?
		partialSum[blockDim.x+t]=input[start+blockDim.x+t]:
		partialSum[blockDim.x+t]=neutralValue;

	//binary tree reduce
	for(unsigned int stride = blockDim.x; stride>=1; stride >>=1)
	{
		__syncthreads();
		if(t < stride)
			partialSum[t] =f(partialSum[t], partialSum[t+stride]);
	}

	if(t==0){
		output[blockIdx.x]=partialSum[0];
	}
}

template<typename FUNC, typename N>
void reduceFunc(FUNC f,const N* X, N* result, size_t length){
	size_t dimSize = B_SIZE;
	size_t gridSize = CEIL_DIV(length,B_SIZE*2);
	size_t numElements;
	N* sumX;

	cudaMalloc((void**) &sumX,sizeof(N)* CEIL_DIV(length,B_SIZE*2));

	f<<<gridSize,dimSize>>>(X,sumX,length);

	while(gridSize>1){
		numElements = gridSize;
		gridSize = CEIL_DIV(gridSize,B_SIZE*2);
		f<<<gridSize,dimSize>>>(sumX,sumX,numElements);
	}
	cudaMemcpy(result,sumX,sizeof(N),cudaMemcpyDeviceToDevice);

	cudaFree(sumX);
}

template<typename FUNC, typename N>
void reduceColFunc(FUNC f,const N* X, N* result, size_t nRows, size_t nCols){
	size_t dimSize = B_SIZE;
	size_t gridSize = CEIL_DIV(nCols,B_SIZE*2);
	size_t numElements;
	N* sumX;

	cudaMalloc((void**) &sumX,sizeof(N)* CEIL_DIV(nCols,B_SIZE*2));

	for(size_t i = 0; i < nRows;++i){
		f<<<gridSize,dimSize>>>(&(X[nCols*i]),sumX,nCols);

		while(gridSize>1){
			numElements = gridSize;
			gridSize = CEIL_DIV(gridSize,B_SIZE*2);
			f<<<gridSize,dimSize>>>(sumX,sumX,numElements);
		}
		cudaMemcpy(&(result[i]),sumX,sizeof(N),cudaMemcpyDeviceToDevice);
		gridSize = CEIL_DIV(nCols,B_SIZE*2);
	}
	cudaFree(sumX);
}

// TODO remove this big overhead
template<typename FUNC, typename N>
N reduceFuncToHostValue(FUNC f, const N* X, size_t length){
	N result = 0;
	N* d_result;

	cudaMalloc((void**) &d_result,sizeof(N));
	f(X,d_result,1,length);
	
	cudaMemcpy(&result, d_result, sizeof(N), cudaMemcpyDeviceToHost);
	cudaFree(d_result);

	return result;
}

/**
 * ZIP
 */
template<typename FUNC, typename N> 
__device__ void	zipFunc(FUNC f, const N* lhs, const N* rhs, N* result, size_t numElements){
	int idx = B_SIZE*blockIdx.x+threadIdx.x;
	if(idx<numElements)
		result[idx]=f(lhs[idx],rhs[idx]);
}

template<typename FUNC, typename N> 
__device__ void	zipFuncSkalar(FUNC f, const N* lhs, const N rhs, N* result, size_t numElements){
	int idx = B_SIZE*blockIdx.x+threadIdx.x;
	if(idx<numElements)
		result[idx]=f(lhs[idx],rhs);
}
